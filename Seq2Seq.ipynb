{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./new_train/new_train\"\n",
    "test_path = \"./new_val_in/new_val_in\"\n",
    "submission_path = \"./sample_submission.csv\"\n",
    "submission_dir = \"./submissions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Val Split\n",
    "TRAIN_SIZE = 0.75\n",
    "VAL_SIZE = 0.25\n",
    "\n",
    "# training config\n",
    "NUM_EPOCH = 5\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ArgoverseDataset(data_path=train_path)\n",
    "test = ArgoverseDataset(data_path=test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(TRAIN_SIZE * len(train))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train, val = torch.utils.data.random_split(train, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.2354e+03, 1.9295e+03, 2.5700e-01, 3.1313e-01],\n",
       "        [3.2359e+03, 1.9298e+03, 4.6091e+00, 3.1098e+00],\n",
       "        [3.2359e+03, 1.9299e+03, 8.8960e-01, 1.0443e+00],\n",
       "        [3.2373e+03, 1.9315e+03, 1.3368e+01, 1.6047e+01],\n",
       "        [3.2378e+03, 1.9317e+03, 5.1234e+00, 2.2956e+00],\n",
       "        [3.2383e+03, 1.9323e+03, 4.6713e+00, 5.8263e+00],\n",
       "        [3.2387e+03, 1.9326e+03, 4.5387e+00, 3.1254e+00],\n",
       "        [3.2391e+03, 1.9327e+03, 3.4147e+00, 1.0973e+00],\n",
       "        [3.2396e+03, 1.9335e+03, 5.7870e+00, 7.9927e+00],\n",
       "        [3.2402e+03, 1.9340e+03, 5.1474e+00, 5.0185e+00],\n",
       "        [3.2406e+03, 1.9344e+03, 4.0382e+00, 3.1020e+00],\n",
       "        [3.2410e+03, 1.9350e+03, 4.8599e+00, 6.0268e+00],\n",
       "        [3.2415e+03, 1.9354e+03, 5.0286e+00, 4.8091e+00],\n",
       "        [3.2420e+03, 1.9357e+03, 4.7979e+00, 2.0820e+00],\n",
       "        [3.2424e+03, 1.9359e+03, 3.4249e+00, 2.8949e+00],\n",
       "        [3.2428e+03, 1.9363e+03, 4.7367e+00, 3.7335e+00],\n",
       "        [3.2431e+03, 1.9365e+03, 2.8242e+00, 1.9251e+00],\n",
       "        [3.2434e+03, 1.9367e+03, 2.6001e+00, 1.8116e+00],\n",
       "        [3.2439e+03, 1.9371e+03, 5.0465e+00, 4.3822e+00]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp, out = [], []\n",
    "    for scene in batch:\n",
    "        agent_idx = np.where(scene[\"agent_id\"] == np.unique(scene[\"track_id\"].flatten()))[0][0]\n",
    "        inp.append(np.hstack([scene['p_in'][agent_idx], scene['v_in'][agent_idx]]))\n",
    "        # out.append(np.hstack([scene['p_out'][agent_idx], scene['v_out'][agent_idx]]))\n",
    "        out.append(scene['p_out'][agent_idx])\n",
    "\n",
    "    inp = torch.FloatTensor(inp)\n",
    "    out = torch.FloatTensor(out)\n",
    "    return [inp, out]\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle = False, collate_fn=my_collate, num_workers=0)\n",
    "val_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle = False, collate_fn=my_collate, num_workers=0)\n",
    "test_loader = DataLoader(test, batch_size=BATCH_SIZE, shuffle = False, collate_fn=my_collate, num_workers=0)\n",
    "exmaple = iter(train_loader)\n",
    "exmaple.next()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>9897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>9905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>9910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>9918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID   v1   v2   v3   v4   v5   v6   v7   v8   v9  ...  v51  v52  v53  \\\n",
       "0     10002  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1     10015  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2     10019  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3     10028  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4      1003  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "3195   9897  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3196     99  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3197   9905  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3198   9910  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3199   9918  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "      v54  v55  v56  v57  v58  v59  v60  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "3195  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3196  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3197  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3198  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3199  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[3200 rows x 61 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(submission_path)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    \"\"\"RMSE Loss\"\"\"\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(yhat, y))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config\n",
    "ROLLOUT_LEN = 30\n",
    "\n",
    "# model config\n",
    "INPUT_SIZE = 4\n",
    "EMBEDDING_SIZE = 8\n",
    "HIDDEN_SIZE = 16\n",
    "OUTPUT_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.linear = nn.Linear(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTMCell(embedding_size, hidden_size)\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden_size):\n",
    "        # Initialize encoder hidden state\n",
    "        return (\n",
    "            torch.zeros(batch_size, hidden_size).to(device),\n",
    "            torch.zeros(batch_size, hidden_size).to(device),\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        init_hidden = self.init_hidden(X.shape[0], self.hidden_size)\n",
    "        \n",
    "        embedded = F.relu(self.linear(X))\n",
    "        hidden_state = self.lstm(embedded, init_hidden)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.linear1 = nn.Linear(output_size, embedding_size)\n",
    "        self.lstm1 = nn.LSTMCell(embedding_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X, encoder_hidden):        \n",
    "        embedded = F.relu(self.linear1(X))\n",
    "        hidden = self.lstm1(embedded, encoder_hidden)\n",
    "        output = self.linear2(hidden[0])\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "encoder = EncoderLSTM(INPUT_SIZE)\n",
    "decoder = DecoderLSTM()\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "loss_fn = RMSELoss()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, batch 0/1609, loss: 23.1653\n",
      "Epoch 1/5, batch 200/1609, loss: 19.4773\n",
      "Epoch 1/5, batch 400/1609, loss: 16.6437\n",
      "Epoch 1/5, batch 600/1609, loss: 16.3441\n",
      "Epoch 1/5, batch 800/1609, loss: 14.7987\n",
      "Epoch 1/5, batch 1000/1609, loss: 13.3119\n",
      "Epoch 1/5, batch 1200/1609, loss: 12.4540\n",
      "Epoch 1/5, batch 1400/1609, loss: 12.5594\n",
      "Epoch 1/5, batch 1600/1609, loss: 12.3229\n",
      "Epoch 2/5, batch 0/1609, loss: 11.7848\n",
      "Epoch 2/5, batch 200/1609, loss: 11.0003\n",
      "Epoch 2/5, batch 400/1609, loss: 9.4281\n",
      "Epoch 2/5, batch 600/1609, loss: 9.9526\n",
      "Epoch 2/5, batch 800/1609, loss: 9.3272\n",
      "Epoch 2/5, batch 1000/1609, loss: 8.2274\n",
      "Epoch 2/5, batch 1200/1609, loss: 7.6766\n",
      "Epoch 2/5, batch 1400/1609, loss: 8.2957\n",
      "Epoch 2/5, batch 1600/1609, loss: 8.0073\n",
      "Epoch 3/5, batch 0/1609, loss: 7.9661\n",
      "Epoch 3/5, batch 200/1609, loss: 7.4366\n",
      "Epoch 3/5, batch 400/1609, loss: 6.4611\n",
      "Epoch 3/5, batch 600/1609, loss: 7.1600\n",
      "Epoch 3/5, batch 800/1609, loss: 6.8339\n",
      "Epoch 3/5, batch 1000/1609, loss: 5.9722\n",
      "Epoch 3/5, batch 1200/1609, loss: 5.6176\n",
      "Epoch 3/5, batch 1400/1609, loss: 6.6717\n",
      "Epoch 3/5, batch 1600/1609, loss: 6.4022\n",
      "Epoch 4/5, batch 0/1609, loss: 6.3404\n",
      "Epoch 4/5, batch 200/1609, loss: 6.1834\n",
      "Epoch 4/5, batch 400/1609, loss: 5.3534\n",
      "Epoch 4/5, batch 600/1609, loss: 5.9270\n",
      "Epoch 4/5, batch 800/1609, loss: 5.8189\n",
      "Epoch 4/5, batch 1000/1609, loss: 5.0588\n",
      "Epoch 4/5, batch 1200/1609, loss: 4.8796\n",
      "Epoch 4/5, batch 1400/1609, loss: 5.9818\n",
      "Epoch 4/5, batch 1600/1609, loss: 5.5801\n",
      "Epoch 5/5, batch 0/1609, loss: 5.5866\n",
      "Epoch 5/5, batch 200/1609, loss: 5.4667\n",
      "Epoch 5/5, batch 400/1609, loss: 4.8513\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    for i_batch, batch_data in enumerate(train_loader):\n",
    "        inp, out = batch_data\n",
    "        inp = inp.to(device)\n",
    "        out = out.to(device)\n",
    "        \n",
    "        # Set to train mode\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Initialize losses\n",
    "        loss = 0\n",
    "\n",
    "        # Encoder\n",
    "        batch_size = inp.shape[0]\n",
    "        input_length = inp.shape[1]\n",
    "        output_length = out.shape[1]\n",
    "        input_shape = inp.shape[2]\n",
    "\n",
    "        # Get relative position\n",
    "        initial_p_in = inp[:, 0, :2].detach().clone()\n",
    "        inp[:, :, :2] = inp[:, :, :2] - initial_p_in[:, None]\n",
    "        \n",
    "        # Encode observed trajectory\n",
    "        for i in range(input_length):\n",
    "            encoder_input = inp[:, i, :]\n",
    "            encoder_hidden = encoder(encoder_input)\n",
    "        \n",
    "        # Initialize decoder input with last coordinate in encoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = encoder_input[:, :2]\n",
    "        decoder_outputs = torch.zeros(out.shape).to(device)\n",
    "\n",
    "        # Decode hidden state in future trajectory\n",
    "        for i in range(output_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, encoder_hidden)\n",
    "            decoder_outputs[:, i, :] = decoder_output\n",
    "\n",
    "            # Update loss\n",
    "            loss += loss_fn(decoder_output[:, :2], out[:, i, :2] - initial_p_in)\n",
    "\n",
    "            # Use own predictions as inputs at next step\n",
    "            decoder_input = decoder_output\n",
    "\n",
    "        # Get average loss for pred_len\n",
    "        loss = loss / output_length\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        if i_batch % 200 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCH}, batch {i_batch}/{len(train_loader)}, loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3416.7327, 1842.6482])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_p_in[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict \n",
    "with torch.no_grad():\n",
    "    for i_batch, batch_data in enumerate(test_loader):\n",
    "        inp, out = batch_data\n",
    "        inp = inp.to(device)\n",
    "        out = out.to(device)\n",
    "\n",
    "        # Set to eval mode\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        # Encoder\n",
    "        batch_size = inp.shape[0]\n",
    "        input_length = inp.shape[1]\n",
    "        output_length = out.shape[1]\n",
    "        input_shape = inp.shape[2]\n",
    "\n",
    "        # Get relative position\n",
    "        initial_p_in = inp[:, 0, :2].detach().clone()\n",
    "        inp[:, :, :2] = inp[:, :, :2] - initial_p_in[:, None]\n",
    "\n",
    "        # Encode observed trajectory\n",
    "        for i in range(input_length):\n",
    "            encoder_input = inp[:, i, :]\n",
    "            encoder_hidden = encoder(encoder_input)\n",
    "\n",
    "        # Initialize decoder input with last coordinate in encoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = encoder_input[:, :2]\n",
    "        decoder_outputs = torch.zeros(out.shape)\n",
    "\n",
    "        # Decode hidden state in future trajectory\n",
    "        for i in range(output_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, encoder_hidden)\n",
    "            decoder_outputs[:, i, :] = decoder_output + initial_p_in\n",
    "\n",
    "            # Use own predictions as inputs at next step\n",
    "            decoder_input = decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make final submission dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def save_submission(df, filename):\n",
    "    filename = filename + \"_\" + str(datetime.now()) + \".csv\"\n",
    "    file_path = os.path.join(submission_dir, filename)\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_submission(df, \"seq2seq(lstm)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
