{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./new_train/new_train\"\n",
    "test_path = \"./new_val_in/new_val_in\"\n",
    "submission_path = \"./sample_submission.csv\"\n",
    "submission_dir = \"./submissions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Val Split\n",
    "TRAIN_SIZE = 0.9\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "# training config\n",
    "NUM_EPOCH = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "EARLY_STOP_MAX = 6\n",
    "\n",
    "# feature engineering configs\n",
    "NEARBY_DISTANCE_THRESHOLD = 50.0  # Distance threshold to call a track as neighbor\n",
    "DEFAULT_MIN_DIST_FRONT_AND_BACK = 100. # default distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ArgoverseDataset(data_path=train_path)\n",
    "test = ArgoverseDataset(data_path=test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(TRAIN_SIZE * len(train))\n",
    "val_size = len(train) - train_size\n",
    "train, val = torch.utils.data.random_split(train, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate social features\n",
    "def get_social_features(scene):\n",
    "    \"\"\"\n",
    "    Extract social features:\n",
    "        1. number of neighbors\n",
    "        2. min front/back distance at each timestamp\n",
    "    \"\"\"\n",
    "    agent_idx = np.where(scene[\"agent_id\"] == np.unique(scene[\"track_id\"].flatten()))[0][0]\n",
    "    social_masks = scene[\"car_mask\"].flatten()\n",
    "    \n",
    "    # remove agent track from social tracks\n",
    "    social_masks[agent_idx] = 0\n",
    "    \n",
    "    # get agent_traj and social_trajs\n",
    "    agent_traj = scene['p_in'][agent_idx]\n",
    "    social_trajs = scene['p_in'][social_masks.astype(bool)]\n",
    "    \n",
    "    # compute social features\n",
    "    num_neighbors = count_num_neighbors(agent_traj, social_trajs)\n",
    "#     min_dist = get_min_distance_front_and_back(agent_traj, social_trajs)\n",
    "#     return np.concatenate((num_neighbors, min_dist), axis=1)\n",
    "    return num_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_num_neighbors(agent_traj, social_trajs):\n",
    "    \"\"\"\n",
    "    Calculate euclidean distance between agent_traj and social_trajs\n",
    "    if distance is less than NEARBY_DISTANCE_THRESHOLD, then num_neighbors++\n",
    "    \n",
    "    Args:\n",
    "        agent_traj (np.array): data for agent trajectory\n",
    "        social_trajs (np.array): array of other agents' trajectories\n",
    "    Returns:\n",
    "        (np.array): \n",
    "    \"\"\"\n",
    "    num_neighbors = []\n",
    "    dist = np.sqrt(\n",
    "        (social_trajs[:, :, 0] - agent_traj[:, 0])**2 \n",
    "        + (social_trajs[:, :, 1] - agent_traj[:, 1])**2\n",
    "    ).T\n",
    "    num_neighbors = np.sum(dist < NEARBY_DISTANCE_THRESHOLD, axis=1)\n",
    "    return num_neighbors.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_distance_front_and_back(agent_traj, social_trajs):\n",
    "    \"\"\"Use minimum distance to the vehicle in front, to the vehicle in back\"\"\"\n",
    "    traj_len = agent_traj.shape[0]\n",
    "    min_distance_front_and_back = np.full(traj_len, DEFAULT_MIN_DIST_FRONT_AND_BACK)\n",
    "    \n",
    "    for i in range(traj_len):\n",
    "        # Agent coordinates\n",
    "        agent_x, agent_y = agent_traj[i, 0], agent_traj[i, 1]\n",
    "\n",
    "#         # Compute distances for all the social tracks\n",
    "#         for social_traj in social_trajs[:, i, :]:\n",
    "#             neigh_x, neigh_y = social_traj[0], social_traj[1]\n",
    "\n",
    "#             # Distance between agent and social\n",
    "#             instant_distance = np.sqrt((agent_x - neigh_x)**2 + (agent_y - neigh_y)**2)\n",
    "\n",
    "#             # If not a neighbor, continue\n",
    "#             if instant_distance > NEARBY_DISTANCE_THRESHOLD:\n",
    "#                 continue\n",
    "                \n",
    "        agent = agent_traj[i]\n",
    "        social = social_trajs[:, i, :]\n",
    "\n",
    "        instant_distance = min(np.sqrt(np.sum((agent - social)**2, axis=1)))\n",
    "        if instant_distance > NEARBY_DISTANCE_THRESHOLD:\n",
    "            continue\n",
    "        else:\n",
    "            min_distance_front_and_back[i] = instant_distance\n",
    "            # Check if the social track is in front or back\n",
    "#             is_front_or_back = get_is_front_or_back(\n",
    "#                 agent_traj[:2, :] if i == 0 else agent_track[:i + 1, :],\n",
    "#                 social_traj\n",
    "#             )\n",
    "            \n",
    "#             if is_front_or_back == \"front\":\n",
    "#                 min_distance_front_and_back[i, 0] = min(\n",
    "#                     min_distance_front_and_back[i, 0], instant_distance)\n",
    "\n",
    "#             elif is_front_or_back == \"back\":\n",
    "#                 min_distance_front_and_back[i, 1] = min(\n",
    "#                     min_distance_front_and_back[i, 1], instant_distance)\n",
    "    return min_distance_front_and_back.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_is_front_or_back(agent_traj, social_p):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_traj = train_0[\"p_in\"][1]\n",
    "social_trajs = train_0[\"p_in\"][2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 19, 2)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_trajs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4858e+02,  2.6593e+03,  9.7674e-01,  1.4160e+01,  4.0000e+00],\n",
       "        [ 1.4835e+02,  2.6601e+03, -2.2618e+00,  7.8510e+00,  4.0000e+00],\n",
       "        [ 1.4840e+02,  2.6615e+03,  4.8918e-01,  1.3997e+01,  4.0000e+00],\n",
       "        [ 1.4835e+02,  2.6627e+03, -5.2795e-01,  1.1960e+01,  4.0000e+00],\n",
       "        [ 1.4836e+02,  2.6636e+03,  1.4605e-01,  9.1254e+00,  4.0000e+00],\n",
       "        [ 1.4843e+02,  2.6645e+03,  6.3606e-01,  9.2585e+00,  4.0000e+00],\n",
       "        [ 1.4844e+02,  2.6658e+03,  1.1599e-01,  1.3361e+01,  4.0000e+00],\n",
       "        [ 1.4831e+02,  2.6670e+03, -1.2448e+00,  1.1686e+01,  4.0000e+00],\n",
       "        [ 1.4830e+02,  2.6683e+03, -1.2394e-01,  1.2613e+01,  5.0000e+00],\n",
       "        [ 1.4844e+02,  2.6695e+03,  1.3973e+00,  1.2108e+01,  5.0000e+00],\n",
       "        [ 1.4833e+02,  2.6706e+03, -1.0835e+00,  1.1160e+01,  5.0000e+00],\n",
       "        [ 1.4817e+02,  2.6711e+03, -1.5964e+00,  5.3164e+00,  5.0000e+00],\n",
       "        [ 1.4830e+02,  2.6727e+03,  1.2815e+00,  1.5362e+01,  5.0000e+00],\n",
       "        [ 1.4836e+02,  2.6738e+03,  5.9413e-01,  1.1479e+01,  5.0000e+00],\n",
       "        [ 1.4828e+02,  2.6748e+03, -7.8823e-01,  1.0023e+01,  5.0000e+00],\n",
       "        [ 1.4830e+02,  2.6759e+03,  1.9972e-01,  1.0399e+01,  5.0000e+00],\n",
       "        [ 1.4830e+02,  2.6770e+03,  7.0564e-03,  1.1443e+01,  5.0000e+00],\n",
       "        [ 1.4834e+02,  2.6781e+03,  3.8312e-01,  1.1111e+01,  5.0000e+00],\n",
       "        [ 1.4838e+02,  2.6794e+03,  4.2782e-01,  1.2719e+01,  5.0000e+00]])"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp, out = [], []\n",
    "    for scene in batch:\n",
    "        agent_idx = np.where(scene[\"agent_id\"] == np.unique(scene[\"track_id\"].flatten()))[0][0]\n",
    "        social_features = get_social_features(scene)\n",
    "        inp.append(np.hstack([scene['p_in'][agent_idx], scene['v_in'][agent_idx], social_features]))\n",
    "        out.append(scene['p_out'][agent_idx])\n",
    "\n",
    "    inp = torch.FloatTensor(inp)\n",
    "    out = torch.FloatTensor(out)\n",
    "    return [inp, out]\n",
    "\n",
    "def my_test_collect(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp, out = [], []\n",
    "    for scene in batch:\n",
    "        agent_idx = np.where(scene[\"agent_id\"] == np.unique(scene[\"track_id\"].flatten()))[0][0]\n",
    "        social_features = get_social_features(scene)\n",
    "        inp.append(np.hstack([scene['p_in'][agent_idx], scene['v_in'][agent_idx], social_features]))\n",
    "        out.append([])\n",
    "\n",
    "    inp = torch.FloatTensor(inp)\n",
    "    out = torch.FloatTensor(out)\n",
    "    return [inp, out]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle = False, collate_fn=my_collate, num_workers=0)\n",
    "val_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle = False, collate_fn=my_collate, num_workers=0)\n",
    "test_loader = DataLoader(test, batch_size=BATCH_SIZE, shuffle = False, collate_fn=my_test_collect, num_workers=0)\n",
    "exmaple = iter(train_loader)\n",
    "exmaple.next()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>9897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>9905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>9910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>9918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID   v1   v2   v3   v4   v5   v6   v7   v8   v9  ...  v51  v52  v53  \\\n",
       "0     10002  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1     10015  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2     10019  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3     10028  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4      1003  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "3195   9897  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3196     99  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3197   9905  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3198   9910  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3199   9918  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "      v54  v55  v56  v57  v58  v59  v60  \n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...   ...  ...  ...  ...  ...  ...  ...  \n",
       "3195  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3196  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3197  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3198  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3199  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[3200 rows x 61 columns]"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(submission_path)\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_position(inp, out=None):\n",
    "    \"\"\"calculate position difference\"\"\"\n",
    "    input_length = inp.shape[1]\n",
    "    p_in_0 = copy.deepcopy(inp[:, 0, :2])\n",
    "    \n",
    "    \n",
    "    if out is not None:\n",
    "        out = copy.deepcopy(out)\n",
    "        output_length = out.shape[1]\n",
    "        for i in range(output_length - 1, 0, -1):\n",
    "            out[:, i, :2] = out[:, i, :2] - out[:, i - 1, :2]\n",
    "        out[:, 0, :2] = out[:, 0, :2] - inp[:, -1, :2]\n",
    "        \n",
    "    for i in range(input_length - 1, 0, -1):\n",
    "        inp[:, i, :2] = inp[:, i, :2] - inp[:, i - 1, :2]\n",
    "    inp[:, 0, :] = 0\n",
    "    \n",
    "    if out is not None:\n",
    "        return inp, out, p_in_0\n",
    "    \n",
    "    return inp, p_in_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absolute_position(inp, out, p_in_0, return_pred_only=False):\n",
    "    \"\"\"position inverse difference\"\"\"\n",
    "    inp[:, 0, :2] = p_in_0\n",
    "    for i in range(1, inp.shape[1]):\n",
    "        inp[:, i, :2] = inp[:, i, :2] + inp[:, i - 1, :2]\n",
    "\n",
    "    out[:, 0, :2] = out[:, 0, :2] + inp[:, -1, :2]\n",
    "    for i in range(1, out.shape[1]):\n",
    "        out[:, i, :2] = out[:, i, :2] + out[:, i - 1, :2]\n",
    "    \n",
    "    if return_pred_only:\n",
    "        return out\n",
    "    \n",
    "    return inp, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    \"\"\"RMSE Loss\"\"\"\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(yhat, y))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config\n",
    "ROLLOUT_LEN = 30\n",
    "\n",
    "# model config\n",
    "INPUT_SIZE = 5\n",
    "EMBEDDING_SIZE = 8\n",
    "HIDDEN_SIZE = 16\n",
    "OUTPUT_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.linear = nn.Linear(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTMCell(embedding_size, hidden_size)\n",
    "\n",
    "    def init_hidden(self, batch_size, hidden_size):\n",
    "        # Initialize encoder hidden state\n",
    "        h_0 = torch.zeros(batch_size, hidden_size).to(device)\n",
    "        c_0 = torch.zeros(batch_size, hidden_size).to(device)\n",
    "        nn.init.xavier_normal_(h_0)\n",
    "        nn.init.xavier_normal_(c_0)\n",
    "        return (h_0, c_0)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        init_hidden = self.init_hidden(X.shape[0], self.hidden_size)\n",
    "        \n",
    "        embedded = F.relu(self.linear(X))\n",
    "        hidden_state = self.lstm(embedded, init_hidden)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.linear1 = nn.Linear(output_size, embedding_size)\n",
    "        self.lstm1 = nn.LSTMCell(embedding_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X, encoder_hidden):        \n",
    "        embedded = F.relu(self.linear1(X))\n",
    "        hidden = self.lstm1(embedded, encoder_hidden)\n",
    "        output = self.linear2(hidden[0])\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validation(val_loader, encoder, decoder, normalize=False):\n",
    "    loss_fn = RMSELoss()\n",
    "    \n",
    "    for i_batch, batch_data in enumerate(val_loader):\n",
    "        inp, out = batch_data\n",
    "        inp = inp.to(device)\n",
    "        out = out.to(device)\n",
    "        \n",
    "        # eval mode\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        # Encoder\n",
    "        batch_size = inp.shape[0]\n",
    "        input_length = inp.shape[1]\n",
    "        output_length = out.shape[1]\n",
    "        input_shape = inp.shape[2]\n",
    "\n",
    "        # Initialize losses\n",
    "        loss = 0\n",
    "\n",
    "        # Get relative position\n",
    "        inp, processed_out, p_in_0 = get_relative_position(inp, out)\n",
    "\n",
    "        # Encode observed trajectory\n",
    "        for i in range(input_length):\n",
    "            encoder_input = inp[:, i, :]\n",
    "            encoder_hidden = encoder(encoder_input)\n",
    "\n",
    "        # Initialize decoder input with last coordinate in encoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = encoder_input[:, :2]\n",
    "\n",
    "        # Decode hidden state in future trajectory\n",
    "        prev_p = p_in_0\n",
    "        for i in range(output_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, encoder_hidden)\n",
    "\n",
    "            # Update loss\n",
    "            pred_out = decoder_output[:, :2] + prev_p\n",
    "            loss += loss_fn(pred_out, out[:, i, :2])\n",
    "            prev_p = pred_out\n",
    "\n",
    "            # Use own predictions as inputs at next step\n",
    "            decoder_input = decoder_output\n",
    "\n",
    "        loss = loss / output_length\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "encoder = EncoderLSTM(5)\n",
    "decoder = DecoderLSTM()\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "loss_fn = RMSELoss()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, batch 0/2897, Training loss: 25.9903\n",
      "Epoch 1/10, batch 400/2897, Training loss: 18.4308\n",
      "Epoch 1/10, batch 800/2897, Training loss: 12.7864\n",
      "Epoch 1/10, batch 1200/2897, Training loss: 13.8592\n",
      "Epoch 1/10, batch 1600/2897, Training loss: 11.2178\n",
      "Epoch 1/10, batch 2000/2897, Training loss: 12.2561\n",
      "Epoch 1/10, batch 2400/2897, Training loss: 9.6018\n",
      "Epoch 1/10, batch 2800/2897, Training loss: 8.3486\n",
      "Epoch 2/10, batch 0/2897, Training loss: 11.8505\n",
      "Epoch 2/10, batch 400/2897, Training loss: 9.0558\n",
      "Epoch 2/10, batch 800/2897, Training loss: 6.4345\n",
      "Epoch 2/10, batch 1200/2897, Training loss: 8.0577\n",
      "Epoch 2/10, batch 1600/2897, Training loss: 5.5263\n",
      "Epoch 2/10, batch 2000/2897, Training loss: 7.1530\n",
      "Epoch 2/10, batch 2400/2897, Training loss: 5.1133\n",
      "Epoch 2/10, batch 2800/2897, Training loss: 4.1518\n",
      "Epoch 3/10, batch 0/2897, Training loss: 7.6639\n",
      "Epoch 3/10, batch 400/2897, Training loss: 5.3218\n",
      "Epoch 3/10, batch 800/2897, Training loss: 4.4960\n",
      "Epoch 3/10, batch 1200/2897, Training loss: 6.0709\n",
      "Epoch 3/10, batch 1600/2897, Training loss: 4.6124\n",
      "Epoch 3/10, batch 2000/2897, Training loss: 5.0847\n",
      "Epoch 3/10, batch 2400/2897, Training loss: 3.4981\n",
      "Epoch 3/10, batch 2800/2897, Training loss: 3.2661\n",
      "Epoch 4/10, batch 0/2897, Training loss: 6.6337\n",
      "Epoch 4/10, batch 400/2897, Training loss: 4.2798\n",
      "Epoch 4/10, batch 800/2897, Training loss: 3.4336\n",
      "Epoch 4/10, batch 1200/2897, Training loss: 4.6223\n",
      "Epoch 4/10, batch 1600/2897, Training loss: 4.1007\n",
      "Epoch 4/10, batch 2000/2897, Training loss: 3.7854\n",
      "Epoch 4/10, batch 2400/2897, Training loss: 2.7740\n",
      "Epoch 4/10, batch 2800/2897, Training loss: 2.7817\n",
      "Epoch 5/10, batch 0/2897, Training loss: 5.5298\n",
      "Epoch 5/10, batch 400/2897, Training loss: 3.3595\n",
      "Epoch 5/10, batch 800/2897, Training loss: 2.7617\n",
      "Epoch 5/10, batch 1200/2897, Training loss: 3.7420\n",
      "Epoch 5/10, batch 1600/2897, Training loss: 3.7464\n",
      "Epoch 5/10, batch 2000/2897, Training loss: 3.3242\n",
      "Epoch 5/10, batch 2400/2897, Training loss: 2.5740\n",
      "Epoch 5/10, batch 2800/2897, Training loss: 2.6724\n",
      "Epoch 6/10, batch 0/2897, Training loss: 5.0652\n",
      "Epoch 6/10, batch 400/2897, Training loss: 3.0362\n",
      "Epoch 6/10, batch 800/2897, Training loss: 2.5823\n",
      "Epoch 6/10, batch 1200/2897, Training loss: 3.3829\n",
      "Epoch 6/10, batch 1600/2897, Training loss: 3.7213\n",
      "Epoch 6/10, batch 2000/2897, Training loss: 3.1148\n",
      "Epoch 6/10, batch 2400/2897, Training loss: 2.5085\n",
      "Epoch 6/10, batch 2800/2897, Training loss: 2.5960\n",
      "Epoch 7/10, batch 0/2897, Training loss: 4.6155\n",
      "Epoch 7/10, batch 400/2897, Training loss: 2.8709\n",
      "Epoch 7/10, batch 800/2897, Training loss: 2.5449\n",
      "Epoch 7/10, batch 1200/2897, Training loss: 3.1910\n",
      "Epoch 7/10, batch 1600/2897, Training loss: 3.7080\n",
      "Epoch 7/10, batch 2000/2897, Training loss: 2.9225\n",
      "Epoch 7/10, batch 2400/2897, Training loss: 2.5117\n",
      "Epoch 7/10, batch 2800/2897, Training loss: 2.6349\n",
      "Epoch 8/10, batch 0/2897, Training loss: 4.3702\n",
      "Epoch 8/10, batch 400/2897, Training loss: 2.7864\n",
      "Epoch 8/10, batch 800/2897, Training loss: 2.4306\n",
      "Epoch 8/10, batch 1200/2897, Training loss: 3.1170\n",
      "Epoch 8/10, batch 1600/2897, Training loss: 3.9164\n",
      "Epoch 8/10, batch 2000/2897, Training loss: 2.7374\n",
      "Epoch 8/10, batch 2400/2897, Training loss: 2.5464\n",
      "Epoch 8/10, batch 2800/2897, Training loss: 2.6944\n",
      "Epoch 9/10, batch 0/2897, Training loss: 4.2321\n",
      "Epoch 9/10, batch 400/2897, Training loss: 2.6679\n",
      "Epoch 9/10, batch 800/2897, Training loss: 2.4206\n",
      "Epoch 9/10, batch 1200/2897, Training loss: 3.0608\n",
      "Epoch 9/10, batch 1600/2897, Training loss: 3.9240\n",
      "Epoch 9/10, batch 2000/2897, Training loss: 2.6282\n",
      "Epoch 9/10, batch 2400/2897, Training loss: 2.5042\n",
      "Epoch 9/10, batch 2800/2897, Training loss: 2.7106\n",
      "Epoch 10/10, batch 0/2897, Training loss: 4.0983\n",
      "Epoch 10/10, batch 400/2897, Training loss: 2.7092\n",
      "Epoch 10/10, batch 800/2897, Training loss: 2.4050\n",
      "Epoch 10/10, batch 1200/2897, Training loss: 2.9777\n",
      "Epoch 10/10, batch 1600/2897, Training loss: 3.8012\n",
      "Epoch 10/10, batch 2000/2897, Training loss: 2.5774\n",
      "Epoch 10/10, batch 2400/2897, Training loss: 2.5028\n",
      "Epoch 10/10, batch 2800/2897, Training loss: 2.7644\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "prev_loss, non_decreasing_loss_cnt = -float(\"inf\"), 0\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    for i_batch, batch_data in enumerate(train_loader):\n",
    "        inp, out = batch_data\n",
    "        inp = inp.to(device)\n",
    "        out = out.to(device)\n",
    "        \n",
    "        # Set to train mode\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Initialize losses\n",
    "        loss = 0\n",
    "\n",
    "        # Encoder\n",
    "        batch_size = inp.shape[0]\n",
    "        input_length = inp.shape[1]  # expected: 19\n",
    "        output_length = out.shape[1]  # expected: 30\n",
    "        input_shape = inp.shape[2]\n",
    "        total_length = input_length + output_length  # expected: 49\n",
    "\n",
    "        # Get relative position  \n",
    "        initial_p_in = inp[:, 0, :2].detach().clone()\n",
    "        inp[:, :, :2] = inp[:, :, :2] - initial_p_in[:, None]\n",
    "        \n",
    "#         inp, processed_out, p_in_0 = get_relative_position(inp, out)\n",
    "#         inp_cols = [0, 1, 4]\n",
    "        # Encode observed trajectory\n",
    "        for i in range(input_length):\n",
    "            encoder_input = inp[:, i, :]\n",
    "            encoder_hidden = encoder(encoder_input)\n",
    "    \n",
    "        # Initialize decoder input with last coordinate in encoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = encoder_input[:, :2]\n",
    "\n",
    "        # Decode hidden state in future trajectory\n",
    "#         prev_p = p_in_0\n",
    "        for i in range(output_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            # Update loss\n",
    "            loss += loss_fn(decoder_output[:, :2], out[:, i, :2] - initial_p_in)\n",
    "            \n",
    "#             pred_out = decoder_output[:, :2] + prev_p\n",
    "#             loss += loss_fn(decoder_output[:, :2], processed_out[:, i, :2])\n",
    "#             prev_p = pred_out\n",
    "\n",
    "            # Use own predictions as inputs at next step\n",
    "            decoder_input = decoder_output\n",
    "\n",
    "        # Get average loss for pred_len\n",
    "        loss = loss / output_length\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "            \n",
    "        if i_batch % 400 == 0:\n",
    "            # print training loss\n",
    "            training_loss = loss.item()\n",
    "            train_losses.append(training_loss)\n",
    "        \n",
    "            # validate \n",
    "#             with torch.no_grad():\n",
    "#                 val_loss = check_validation(val_loader, encoder, decoder, normalize=False)\n",
    "#                 val_losses.append(val_loss.item())  \n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{NUM_EPOCH}, batch {i_batch}/{len(train_loader)}, \"\\\n",
    "                    + f\"Training loss: {training_loss:.4f}\")#\\\n",
    "                    #  + f\", Val loss: {val_loss.item():.4f}\")\n",
    "            \n",
    "            # early stop\n",
    "#             if val_loss.item() > prev_loss:\n",
    "#                 non_decreasing_loss_cnt += 1\n",
    "#             else:\n",
    "#                 non_decreasing_loss_cnt = 0\n",
    "#                 prev_loss = val_loss.item()\n",
    "            \n",
    "#             if non_decreasing_loss_cnt >= EARLY_STOP_MAX:\n",
    "#                 break \n",
    "                \n",
    "#     if non_decreasing_loss_cnt >= EARLY_STOP_MAX:\n",
    "#         break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def show_sample_batch(sample_batch, agent_id):\n",
    "    \"\"\"visualize the trajectory for a batch of samples with a randon agent\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1])\n",
    "        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fig, axs = plt.subplots(1, 1, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "agent_id = 44\n",
    "\n",
    "plt.scatter(real_inp[agent_id].cpu().numpy()[:, 0], real_inp[agent_id].cpu().numpy()[:, 1], label=\"p_in\")\n",
    "plt.scatter(out[agent_id].cpu().numpy()[:, 0], out[agent_id].cpu().numpy()[:, 1], label=\"actual_p_out\")\n",
    "plt.scatter(decoder_outputs[agent_id].cpu().numpy()[:, 0], decoder_outputs[agent_id].cpu().numpy()[:, 1], label=\"predicted_p_out\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Predicted Trajectory vs. Actual Trajectory\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred_p_out = []\n",
    "all_zeros_ix = []\n",
    "real_inp = None\n",
    "with torch.no_grad():\n",
    "    for i_batch, batch_data in enumerate(train_loader):\n",
    "        inp, out = batch_data\n",
    "        inp = inp.to(device)\n",
    "                    \n",
    "        # Set to eval mode\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        # Encoder\n",
    "        batch_size = inp.shape[0]\n",
    "        input_length = inp.shape[1]\n",
    "        input_shape = inp.shape[2]\n",
    "\n",
    "        # Get relative position\n",
    "        initial_p_in = inp[:, 0, :2].detach().clone()\n",
    "        real_inp = inp.detach().clone()\n",
    "        inp[:, :, :2] = inp[:, :, :2] - initial_p_in[:, None]\n",
    "#         inp, p_in_0 = get_relative_position(inp)\n",
    "        \n",
    "        # Encode observed trajectory\n",
    "        for i in range(input_length):\n",
    "            encoder_input = inp[:, i, :]\n",
    "            encoder_hidden = encoder(encoder_input)\n",
    "\n",
    "        # Initialize decoder input with last coordinate in encoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = encoder_input[:, :2]\n",
    "        decoder_outputs = torch.zeros((len(inp), 30, 2))\n",
    "\n",
    "        # Decode hidden state in future trajectory\n",
    "        for i in range(30):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            decoder_outputs[:, i, :] = decoder_output + initial_p_in\n",
    "\n",
    "            # Use own predictions as inputs at next step\n",
    "            decoder_input = decoder_output\n",
    "            \n",
    "        # predicted_p_out\n",
    "#         out = get_absolute_position(inp, decoder_outputs.to(device), p_in_0, return_pred_only=True)\n",
    "        pred_p_out.append(decoder_outputs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1715.4297,  337.4276],\n",
       "         [1715.7571,  338.4755],\n",
       "         [1717.0173,  339.1839],\n",
       "         ...,\n",
       "         [1739.0333,  359.7098],\n",
       "         [1740.0820,  360.7204],\n",
       "         [1741.1708,  361.7773]],\n",
       "\n",
       "        [[ 724.4598, 1230.1902],\n",
       "         [ 724.4282, 1229.7971],\n",
       "         [ 724.4906, 1229.3918],\n",
       "         ...,\n",
       "         [ 722.8076, 1220.7710],\n",
       "         [ 722.7488, 1220.3905],\n",
       "         [ 722.6932, 1220.0105]],\n",
       "\n",
       "        [[ 574.0156, 1244.2719],\n",
       "         [ 574.3720, 1244.1730],\n",
       "         [ 574.5878, 1243.8409],\n",
       "         ...,\n",
       "         [ 581.5117, 1238.9880],\n",
       "         [ 581.8642, 1238.8501],\n",
       "         [ 582.2226, 1238.7168]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1754.9185,  444.3149],\n",
       "         [1754.7728,  444.2262],\n",
       "         [1754.5679,  444.4013],\n",
       "         ...,\n",
       "         [1747.7212,  451.6100],\n",
       "         [1747.5109,  451.8706],\n",
       "         [1747.3057,  452.1302]],\n",
       "\n",
       "        [[ 574.0685, 1288.6942],\n",
       "         [ 573.9257, 1288.3611],\n",
       "         [ 573.9498, 1288.0721],\n",
       "         ...,\n",
       "         [ 571.5893, 1280.2318],\n",
       "         [ 571.4891, 1279.8473],\n",
       "         [ 571.3922, 1279.4607]],\n",
       "\n",
       "        [[ 585.1806, 1164.6412],\n",
       "         [ 585.3699, 1163.6747],\n",
       "         [ 585.3889, 1162.5802],\n",
       "         ...,\n",
       "         [ 586.1758, 1137.0068],\n",
       "         [ 586.2069, 1136.0472],\n",
       "         [ 586.2361, 1135.0953]]])"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_p_out = torch.FloatTensor(len(val_loader), 30, 2)#.to(device)\n",
    "torch.cat(pred_p_out, out=predicted_p_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1715.429688</td>\n",
       "      <td>337.427612</td>\n",
       "      <td>1715.757080</td>\n",
       "      <td>338.475494</td>\n",
       "      <td>1717.017334</td>\n",
       "      <td>339.183899</td>\n",
       "      <td>1717.715698</td>\n",
       "      <td>339.929993</td>\n",
       "      <td>1718.682129</td>\n",
       "      <td>340.654114</td>\n",
       "      <td>...</td>\n",
       "      <td>1737.064209</td>\n",
       "      <td>357.836304</td>\n",
       "      <td>1738.027832</td>\n",
       "      <td>358.749359</td>\n",
       "      <td>1739.033325</td>\n",
       "      <td>359.709839</td>\n",
       "      <td>1740.082031</td>\n",
       "      <td>360.720428</td>\n",
       "      <td>1741.170776</td>\n",
       "      <td>361.777344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>724.459839</td>\n",
       "      <td>1230.190186</td>\n",
       "      <td>724.428162</td>\n",
       "      <td>1229.797119</td>\n",
       "      <td>724.490601</td>\n",
       "      <td>1229.391846</td>\n",
       "      <td>724.481689</td>\n",
       "      <td>1229.032227</td>\n",
       "      <td>724.444458</td>\n",
       "      <td>1228.704956</td>\n",
       "      <td>...</td>\n",
       "      <td>722.934387</td>\n",
       "      <td>1221.530884</td>\n",
       "      <td>722.869507</td>\n",
       "      <td>1221.151245</td>\n",
       "      <td>722.807556</td>\n",
       "      <td>1220.770996</td>\n",
       "      <td>722.748779</td>\n",
       "      <td>1220.390503</td>\n",
       "      <td>722.693237</td>\n",
       "      <td>1220.010498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>574.015625</td>\n",
       "      <td>1244.271851</td>\n",
       "      <td>574.372009</td>\n",
       "      <td>1244.172974</td>\n",
       "      <td>574.587769</td>\n",
       "      <td>1243.840942</td>\n",
       "      <td>574.773682</td>\n",
       "      <td>1243.532959</td>\n",
       "      <td>574.956360</td>\n",
       "      <td>1243.254761</td>\n",
       "      <td>...</td>\n",
       "      <td>580.823120</td>\n",
       "      <td>1239.276978</td>\n",
       "      <td>581.164673</td>\n",
       "      <td>1239.130371</td>\n",
       "      <td>581.511658</td>\n",
       "      <td>1238.988037</td>\n",
       "      <td>581.864197</td>\n",
       "      <td>1238.850098</td>\n",
       "      <td>582.222595</td>\n",
       "      <td>1238.716797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1692.028320</td>\n",
       "      <td>316.085083</td>\n",
       "      <td>1692.652466</td>\n",
       "      <td>316.918488</td>\n",
       "      <td>1693.527344</td>\n",
       "      <td>317.503113</td>\n",
       "      <td>1694.230103</td>\n",
       "      <td>318.116608</td>\n",
       "      <td>1694.986572</td>\n",
       "      <td>318.734253</td>\n",
       "      <td>...</td>\n",
       "      <td>1709.485352</td>\n",
       "      <td>332.452118</td>\n",
       "      <td>1710.134521</td>\n",
       "      <td>333.067535</td>\n",
       "      <td>1710.785767</td>\n",
       "      <td>333.679871</td>\n",
       "      <td>1711.440918</td>\n",
       "      <td>334.290405</td>\n",
       "      <td>1712.102295</td>\n",
       "      <td>334.900604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2122.706299</td>\n",
       "      <td>676.172241</td>\n",
       "      <td>2121.003662</td>\n",
       "      <td>675.390259</td>\n",
       "      <td>2119.952148</td>\n",
       "      <td>674.025208</td>\n",
       "      <td>2118.684814</td>\n",
       "      <td>672.776611</td>\n",
       "      <td>2117.351562</td>\n",
       "      <td>671.626587</td>\n",
       "      <td>...</td>\n",
       "      <td>2091.059326</td>\n",
       "      <td>648.720581</td>\n",
       "      <td>2089.739990</td>\n",
       "      <td>647.489075</td>\n",
       "      <td>2088.513916</td>\n",
       "      <td>646.321106</td>\n",
       "      <td>2087.398682</td>\n",
       "      <td>645.222900</td>\n",
       "      <td>2086.429688</td>\n",
       "      <td>644.205383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            v1           v2           v3           v4           v5  \\\n",
       "0  1715.429688   337.427612  1715.757080   338.475494  1717.017334   \n",
       "1   724.459839  1230.190186   724.428162  1229.797119   724.490601   \n",
       "2   574.015625  1244.271851   574.372009  1244.172974   574.587769   \n",
       "3  1692.028320   316.085083  1692.652466   316.918488  1693.527344   \n",
       "4  2122.706299   676.172241  2121.003662   675.390259  2119.952148   \n",
       "\n",
       "            v6           v7           v8           v9          v10  ...  \\\n",
       "0   339.183899  1717.715698   339.929993  1718.682129   340.654114  ...   \n",
       "1  1229.391846   724.481689  1229.032227   724.444458  1228.704956  ...   \n",
       "2  1243.840942   574.773682  1243.532959   574.956360  1243.254761  ...   \n",
       "3   317.503113  1694.230103   318.116608  1694.986572   318.734253  ...   \n",
       "4   674.025208  2118.684814   672.776611  2117.351562   671.626587  ...   \n",
       "\n",
       "           v51          v52          v53          v54          v55  \\\n",
       "0  1737.064209   357.836304  1738.027832   358.749359  1739.033325   \n",
       "1   722.934387  1221.530884   722.869507  1221.151245   722.807556   \n",
       "2   580.823120  1239.276978   581.164673  1239.130371   581.511658   \n",
       "3  1709.485352   332.452118  1710.134521   333.067535  1710.785767   \n",
       "4  2091.059326   648.720581  2089.739990   647.489075  2088.513916   \n",
       "\n",
       "           v56          v57          v58          v59          v60  \n",
       "0   359.709839  1740.082031   360.720428  1741.170776   361.777344  \n",
       "1  1220.770996   722.748779  1220.390503   722.693237  1220.010498  \n",
       "2  1238.988037   581.864197  1238.850098   582.222595  1238.716797  \n",
       "3   333.679871  1711.440918   334.290405  1712.102295   334.900604  \n",
       "4   646.321106  2087.398682   645.222900  2086.429688   644.205383  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make final submission dataframe\n",
    "df = pd.DataFrame(predicted_p_out.view(3200, -1).cpu().numpy(), columns=[f\"v{i}\" for i in range(1, 61)])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v51</th>\n",
       "      <th>v52</th>\n",
       "      <th>v53</th>\n",
       "      <th>v54</th>\n",
       "      <th>v55</th>\n",
       "      <th>v56</th>\n",
       "      <th>v57</th>\n",
       "      <th>v58</th>\n",
       "      <th>v59</th>\n",
       "      <th>v60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10002</td>\n",
       "      <td>1715.429688</td>\n",
       "      <td>337.427612</td>\n",
       "      <td>1715.757080</td>\n",
       "      <td>338.475494</td>\n",
       "      <td>1717.017334</td>\n",
       "      <td>339.183899</td>\n",
       "      <td>1717.715698</td>\n",
       "      <td>339.929993</td>\n",
       "      <td>1718.682129</td>\n",
       "      <td>...</td>\n",
       "      <td>1737.064209</td>\n",
       "      <td>357.836304</td>\n",
       "      <td>1738.027832</td>\n",
       "      <td>358.749359</td>\n",
       "      <td>1739.033325</td>\n",
       "      <td>359.709839</td>\n",
       "      <td>1740.082031</td>\n",
       "      <td>360.720428</td>\n",
       "      <td>1741.170776</td>\n",
       "      <td>361.777344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10015</td>\n",
       "      <td>724.459839</td>\n",
       "      <td>1230.190186</td>\n",
       "      <td>724.428162</td>\n",
       "      <td>1229.797119</td>\n",
       "      <td>724.490601</td>\n",
       "      <td>1229.391846</td>\n",
       "      <td>724.481689</td>\n",
       "      <td>1229.032227</td>\n",
       "      <td>724.444458</td>\n",
       "      <td>...</td>\n",
       "      <td>722.934387</td>\n",
       "      <td>1221.530884</td>\n",
       "      <td>722.869507</td>\n",
       "      <td>1221.151245</td>\n",
       "      <td>722.807556</td>\n",
       "      <td>1220.770996</td>\n",
       "      <td>722.748779</td>\n",
       "      <td>1220.390503</td>\n",
       "      <td>722.693237</td>\n",
       "      <td>1220.010498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10019</td>\n",
       "      <td>574.015625</td>\n",
       "      <td>1244.271851</td>\n",
       "      <td>574.372009</td>\n",
       "      <td>1244.172974</td>\n",
       "      <td>574.587769</td>\n",
       "      <td>1243.840942</td>\n",
       "      <td>574.773682</td>\n",
       "      <td>1243.532959</td>\n",
       "      <td>574.956360</td>\n",
       "      <td>...</td>\n",
       "      <td>580.823120</td>\n",
       "      <td>1239.276978</td>\n",
       "      <td>581.164673</td>\n",
       "      <td>1239.130371</td>\n",
       "      <td>581.511658</td>\n",
       "      <td>1238.988037</td>\n",
       "      <td>581.864197</td>\n",
       "      <td>1238.850098</td>\n",
       "      <td>582.222595</td>\n",
       "      <td>1238.716797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10028</td>\n",
       "      <td>1692.028320</td>\n",
       "      <td>316.085083</td>\n",
       "      <td>1692.652466</td>\n",
       "      <td>316.918488</td>\n",
       "      <td>1693.527344</td>\n",
       "      <td>317.503113</td>\n",
       "      <td>1694.230103</td>\n",
       "      <td>318.116608</td>\n",
       "      <td>1694.986572</td>\n",
       "      <td>...</td>\n",
       "      <td>1709.485352</td>\n",
       "      <td>332.452118</td>\n",
       "      <td>1710.134521</td>\n",
       "      <td>333.067535</td>\n",
       "      <td>1710.785767</td>\n",
       "      <td>333.679871</td>\n",
       "      <td>1711.440918</td>\n",
       "      <td>334.290405</td>\n",
       "      <td>1712.102295</td>\n",
       "      <td>334.900604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003</td>\n",
       "      <td>2122.706299</td>\n",
       "      <td>676.172241</td>\n",
       "      <td>2121.003662</td>\n",
       "      <td>675.390259</td>\n",
       "      <td>2119.952148</td>\n",
       "      <td>674.025208</td>\n",
       "      <td>2118.684814</td>\n",
       "      <td>672.776611</td>\n",
       "      <td>2117.351562</td>\n",
       "      <td>...</td>\n",
       "      <td>2091.059326</td>\n",
       "      <td>648.720581</td>\n",
       "      <td>2089.739990</td>\n",
       "      <td>647.489075</td>\n",
       "      <td>2088.513916</td>\n",
       "      <td>646.321106</td>\n",
       "      <td>2087.398682</td>\n",
       "      <td>645.222900</td>\n",
       "      <td>2086.429688</td>\n",
       "      <td>644.205383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>9897</td>\n",
       "      <td>256.148010</td>\n",
       "      <td>805.357666</td>\n",
       "      <td>256.549683</td>\n",
       "      <td>805.154907</td>\n",
       "      <td>256.773071</td>\n",
       "      <td>804.689514</td>\n",
       "      <td>257.015228</td>\n",
       "      <td>804.292603</td>\n",
       "      <td>257.239197</td>\n",
       "      <td>...</td>\n",
       "      <td>262.705597</td>\n",
       "      <td>797.522583</td>\n",
       "      <td>263.008453</td>\n",
       "      <td>797.190552</td>\n",
       "      <td>263.318542</td>\n",
       "      <td>796.856140</td>\n",
       "      <td>263.636444</td>\n",
       "      <td>796.519531</td>\n",
       "      <td>263.962769</td>\n",
       "      <td>796.180603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>99</td>\n",
       "      <td>588.004211</td>\n",
       "      <td>1153.406494</td>\n",
       "      <td>587.949707</td>\n",
       "      <td>1152.336060</td>\n",
       "      <td>587.925110</td>\n",
       "      <td>1151.283325</td>\n",
       "      <td>587.925171</td>\n",
       "      <td>1150.436157</td>\n",
       "      <td>587.921509</td>\n",
       "      <td>...</td>\n",
       "      <td>588.277527</td>\n",
       "      <td>1133.508667</td>\n",
       "      <td>588.309631</td>\n",
       "      <td>1132.600586</td>\n",
       "      <td>588.339478</td>\n",
       "      <td>1131.682129</td>\n",
       "      <td>588.367188</td>\n",
       "      <td>1130.756470</td>\n",
       "      <td>588.393250</td>\n",
       "      <td>1129.828247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>9905</td>\n",
       "      <td>1754.918457</td>\n",
       "      <td>444.314941</td>\n",
       "      <td>1754.772827</td>\n",
       "      <td>444.226196</td>\n",
       "      <td>1754.567871</td>\n",
       "      <td>444.401337</td>\n",
       "      <td>1754.277466</td>\n",
       "      <td>444.683472</td>\n",
       "      <td>1753.953369</td>\n",
       "      <td>...</td>\n",
       "      <td>1748.157349</td>\n",
       "      <td>451.085114</td>\n",
       "      <td>1747.936768</td>\n",
       "      <td>451.348236</td>\n",
       "      <td>1747.721191</td>\n",
       "      <td>451.610046</td>\n",
       "      <td>1747.510864</td>\n",
       "      <td>451.870636</td>\n",
       "      <td>1747.305664</td>\n",
       "      <td>452.130219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>9910</td>\n",
       "      <td>574.068542</td>\n",
       "      <td>1288.694214</td>\n",
       "      <td>573.925720</td>\n",
       "      <td>1288.361084</td>\n",
       "      <td>573.949829</td>\n",
       "      <td>1288.072144</td>\n",
       "      <td>573.921387</td>\n",
       "      <td>1287.795288</td>\n",
       "      <td>573.869873</td>\n",
       "      <td>...</td>\n",
       "      <td>571.798767</td>\n",
       "      <td>1280.992554</td>\n",
       "      <td>571.692627</td>\n",
       "      <td>1280.613647</td>\n",
       "      <td>571.589294</td>\n",
       "      <td>1280.231812</td>\n",
       "      <td>571.489075</td>\n",
       "      <td>1279.847290</td>\n",
       "      <td>571.392151</td>\n",
       "      <td>1279.460693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>9918</td>\n",
       "      <td>585.180603</td>\n",
       "      <td>1164.641235</td>\n",
       "      <td>585.369873</td>\n",
       "      <td>1163.674683</td>\n",
       "      <td>585.388855</td>\n",
       "      <td>1162.580200</td>\n",
       "      <td>585.460266</td>\n",
       "      <td>1161.519775</td>\n",
       "      <td>585.530151</td>\n",
       "      <td>...</td>\n",
       "      <td>586.120239</td>\n",
       "      <td>1138.970947</td>\n",
       "      <td>586.145874</td>\n",
       "      <td>1137.979980</td>\n",
       "      <td>586.175842</td>\n",
       "      <td>1137.006836</td>\n",
       "      <td>586.206909</td>\n",
       "      <td>1136.047241</td>\n",
       "      <td>586.236084</td>\n",
       "      <td>1135.095337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID           v1           v2           v3           v4           v5  \\\n",
       "0     10002  1715.429688   337.427612  1715.757080   338.475494  1717.017334   \n",
       "1     10015   724.459839  1230.190186   724.428162  1229.797119   724.490601   \n",
       "2     10019   574.015625  1244.271851   574.372009  1244.172974   574.587769   \n",
       "3     10028  1692.028320   316.085083  1692.652466   316.918488  1693.527344   \n",
       "4      1003  2122.706299   676.172241  2121.003662   675.390259  2119.952148   \n",
       "...     ...          ...          ...          ...          ...          ...   \n",
       "3195   9897   256.148010   805.357666   256.549683   805.154907   256.773071   \n",
       "3196     99   588.004211  1153.406494   587.949707  1152.336060   587.925110   \n",
       "3197   9905  1754.918457   444.314941  1754.772827   444.226196  1754.567871   \n",
       "3198   9910   574.068542  1288.694214   573.925720  1288.361084   573.949829   \n",
       "3199   9918   585.180603  1164.641235   585.369873  1163.674683   585.388855   \n",
       "\n",
       "               v6           v7           v8           v9  ...          v51  \\\n",
       "0      339.183899  1717.715698   339.929993  1718.682129  ...  1737.064209   \n",
       "1     1229.391846   724.481689  1229.032227   724.444458  ...   722.934387   \n",
       "2     1243.840942   574.773682  1243.532959   574.956360  ...   580.823120   \n",
       "3      317.503113  1694.230103   318.116608  1694.986572  ...  1709.485352   \n",
       "4      674.025208  2118.684814   672.776611  2117.351562  ...  2091.059326   \n",
       "...           ...          ...          ...          ...  ...          ...   \n",
       "3195   804.689514   257.015228   804.292603   257.239197  ...   262.705597   \n",
       "3196  1151.283325   587.925171  1150.436157   587.921509  ...   588.277527   \n",
       "3197   444.401337  1754.277466   444.683472  1753.953369  ...  1748.157349   \n",
       "3198  1288.072144   573.921387  1287.795288   573.869873  ...   571.798767   \n",
       "3199  1162.580200   585.460266  1161.519775   585.530151  ...   586.120239   \n",
       "\n",
       "              v52          v53          v54          v55          v56  \\\n",
       "0      357.836304  1738.027832   358.749359  1739.033325   359.709839   \n",
       "1     1221.530884   722.869507  1221.151245   722.807556  1220.770996   \n",
       "2     1239.276978   581.164673  1239.130371   581.511658  1238.988037   \n",
       "3      332.452118  1710.134521   333.067535  1710.785767   333.679871   \n",
       "4      648.720581  2089.739990   647.489075  2088.513916   646.321106   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "3195   797.522583   263.008453   797.190552   263.318542   796.856140   \n",
       "3196  1133.508667   588.309631  1132.600586   588.339478  1131.682129   \n",
       "3197   451.085114  1747.936768   451.348236  1747.721191   451.610046   \n",
       "3198  1280.992554   571.692627  1280.613647   571.589294  1280.231812   \n",
       "3199  1138.970947   586.145874  1137.979980   586.175842  1137.006836   \n",
       "\n",
       "              v57          v58          v59          v60  \n",
       "0     1740.082031   360.720428  1741.170776   361.777344  \n",
       "1      722.748779  1220.390503   722.693237  1220.010498  \n",
       "2      581.864197  1238.850098   582.222595  1238.716797  \n",
       "3     1711.440918   334.290405  1712.102295   334.900604  \n",
       "4     2087.398682   645.222900  2086.429688   644.205383  \n",
       "...           ...          ...          ...          ...  \n",
       "3195   263.636444   796.519531   263.962769   796.180603  \n",
       "3196   588.367188  1130.756470   588.393250  1129.828247  \n",
       "3197  1747.510864   451.870636  1747.305664   452.130219  \n",
       "3198   571.489075  1279.847290   571.392151  1279.460693  \n",
       "3199   586.206909  1136.047241   586.236084  1135.095337  \n",
       "\n",
       "[3200 rows x 61 columns]"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ID'] = submission['ID']\n",
    "cols = df.columns[-1:].tolist() + df.columns[:-1].tolist()\n",
    "df = df[cols]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def save_submission(df, filename):\n",
    "    filename = filename + \"_\" + str(datetime.now()) + \".csv\"\n",
    "    file_path = os.path.join(submission_dir, filename)\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_submission(df, \"LSTM + Social(num_neighbors) + relave_position + epoch10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
